{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5305eae",
   "metadata": {},
   "source": [
    "\n",
    "Intelligent ChatBot built with Microsoft's DialoGPT transformer to make conversations with human users!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36968e",
   "metadata": {},
   "source": [
    "What is a chatbot?¶\n",
    "A ChatBot is a kind of virtual assistant that can build conversations with human users! A Chatting Robot. Building a chatbot is one of the popular tasks in Natural Language Processing.\n",
    "Are all chatbots the same?\n",
    "Chatbots fall under three common categories:\n",
    "1. Rule-based chatbots\n",
    "2. Retrieval-based chatbots\n",
    "3. Intelligent chatbots\n",
    "Rule-based chatbots\n",
    "These bots respond to users' inputs based on certain pre-specified rules. For instance, these rules can be defined as if-elif-else statements. While writing rules for these chatbots, it is important to expect all possible user inputs, else the bot may fail to answer properly. Hence, rule-based chatbots do not possess any cognitive skills.\n",
    "Retrieval-based chatbots\n",
    "These bots respond to users' inputs by retrieving the most relevant information from the given text document. The most relevant information can be determined by Natural Language Processing with a scoring system such as cosine-similarity-score. Though these bots use NLP to do conversations, they lack cognitive skills to match a real human chatting companion.\n",
    "Intelligent AI chatbots\n",
    "These bots respond to users' inputs after understanding the inputs, as humans do. These bots are trained with a Machine Learning Model on a large training dataset of human conversations. These bots are cognitive to match a human in conversing. Amazon's Alexa, Apple's Siri fall under this category. Further, most of these bots can make conversations based on the preceding chat texts.\n",
    "In this Article?\n",
    "This article describes building an intelligent AI chatbot based on the famous transformer architecture - Microsoft's DialoGPT. According to Hugging Face's model card, DialoGPT is a State-Of-The-Art large-scale pretrained dialogue response generation model for multiturn conversations. The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f14eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013fc0be",
   "metadata": {},
   "source": [
    "#Download Microsoft's DialoGPT model and tokenizer\n",
    "The Hugging Face checkpoint for the model and its tokenizer is \"microsoft/DialoGPT-medium\"¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d159d222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000a8e9c86ce4526846dda2ef24e41bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/130 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrut\\Anaconda\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shrut\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a975117ef146989fbb13f3acf0841d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21268606d8ea42a195185adb1ab31d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf93298a64e49bbaffbab4b91a6bce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96845cd73b3240c39097cc99e7116831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/863M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#checkpoint\n",
    "checkpoint='microsoft/DialoGPT-medium'\n",
    "# download and cache tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# download and cache pre-trained model\n",
    "model= AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d6501",
   "metadata": {},
   "source": [
    "# A ChatBot class"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b56ec4e",
   "metadata": {},
   "source": [
    "# Build a ChatBot class with all necessary modules to make a complete conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f935cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot():\n",
    "    #initilize\n",
    "    def __init__(self):\n",
    "        # once chat starts, the history will be stored for chat continuity\n",
    "        self.chat_history_ids=None\n",
    "        #make input ids global to use them anywhere within the object \n",
    "        self.bot_input_ids=None\n",
    "        # a flag to check whether to end the conversation\n",
    "        self.end_chat=False\n",
    "        # greet while starting\n",
    "        self.welcome()\n",
    "        \n",
    "    def welcome(self):\n",
    "        print(\"Initializing ChatBot...\")\n",
    "        # some time to get user ready\n",
    "        time.sleep(2)\n",
    "        print('Type \"bye\" or \"quit\" or \"exit\" to end chat \\n')\n",
    "        # give time to read what has been printed\n",
    "        time.sleep(3)\n",
    "        # Greet and introduce\n",
    "        greeting = np.random.choice([\n",
    "            \"Welcome, I am ChatBot, here for your kind service\",\n",
    "            \"Hey, Great day! I am your virtual assistant\",\n",
    "            \"Hello, it's my pleasure meeting you\",\n",
    "            \"Hi, I am a ChatBot. Let's chat!\"\n",
    "        ])\n",
    "        print(\"ChatBot >>  \"+ greeting)\n",
    "    def user_input(self):\n",
    "        # receive input from user\n",
    "        text = input(\"User >> \")\n",
    "        # end conversation if user wishes so\n",
    "        if text.lower().strip() in ['bye', 'quit', 'exit']:\n",
    "            # turn flag on \n",
    "            self.end_chat=True\n",
    "            # a closing comment\n",
    "            print('ChatBot >> See you soon!  Bye!')\n",
    "            time.sleep(1)\n",
    "            print('\\nQuitting ChatBot......')\n",
    "        else:\n",
    "            # continue chat, preprocess input text\n",
    "            # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "            self.new_user_input_ids = tokenizer.encode(text + tokenizer.eos_token, \\\n",
    "                                                       return_tensors='pt')\n",
    "    def bot_response(self):\n",
    "        # append the new user input tokens to the chat history\n",
    "        # if chat has already begun\n",
    "        if self.chat_history_ids is not None:\n",
    "            self.bot_input_ids=torch.cat([self.chat_history_ids, self.new_user_input_ids], dim=-1)\n",
    "        else:\n",
    "            # if first entry, initialize bot_input_ids\n",
    "            self.bot_input_ids = self.new_user_input_ids\n",
    "            # define the new chat_history_ids based on the preceding chat\n",
    "            # generated a response while limiting the total chat history to 1000 tokens, \n",
    "            self.chat_history_ids = model.generate(self.bot_input_ids, max_length=1000, \\\n",
    "                                               pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "            # last ouput tokens from bot\n",
    "            response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[-1]:][0], \\\n",
    "                               skip_special_tokens=True)\n",
    "\n",
    "            # in case, bot fails to answer\n",
    "            if response == \"\":\n",
    "                response = self.random_response()\n",
    "                # print bot response\n",
    "                print('ChatBot >>  '+ response)\n",
    "        \n",
    "    # in case there is no response from model\n",
    "    def random_response(self):\n",
    "        i = -1\n",
    "        response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[i]:][0], \\\n",
    "                               skip_special_tokens=True)\n",
    "        # iterate over history backwards to find the last token\n",
    "        while response == '':\n",
    "            i = i-1\n",
    "            response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[i]:][0], \\\n",
    "                               skip_special_tokens=True)\n",
    "        # if it is a question, answer suitably\n",
    "        if response.strip() == '?':\n",
    "            reply = np.random.choice([\"I don't know\", \n",
    "                                     \"I am not sure\"])\n",
    "        # not a question? answer suitably\n",
    "        else:\n",
    "            reply = np.random.choice([\"Great\", \n",
    "                                      \"Fine. What's up?\", \n",
    "                                      \"Okay\"\n",
    "                                     ])\n",
    "        return reply\n",
    "                                                                                    \n",
    "                                                                                                 \n",
    "                                                                                                 \n",
    "        \n",
    "         \n",
    "         \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a9b9c6",
   "metadata": {},
   "source": [
    "# Happy Chatting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bee6ed3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChatBot...\n",
      "Type \"bye\" or \"quit\" or \"exit\" to end chat \n",
      "\n",
      "ChatBot >>  Hello, it's my pleasure meeting you\n",
      "User >> how are you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User >> good\n",
      "User >> How are you doing\n",
      "User >> quit\n",
      "ChatBot >> See you soon!  Bye!\n",
      "\n",
      "Quitting ChatBot......\n"
     ]
    }
   ],
   "source": [
    "# build a ChatBot object\n",
    "bot = ChatBot()\n",
    "# start chatting\n",
    "while True:\n",
    "    # receive user input\n",
    "    bot.user_input()\n",
    "    # check whether to end chat\n",
    "    if bot.end_chat:\n",
    "        break\n",
    "    # output bot response\n",
    "    bot.bot_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbc2b03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
